# **Towards Practical Industrial Anomaly Detection** [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

As more and more outstanding vision-language-based policies emerge, this repository aims to organize and showcase the state-of-the-art technologies in robot learning, including vision-language-action (VLA) models, vision-language-navigation (VLN) models, vision-action (VA) models and other MLLM-based embodied learning. We hope that in the near future, robotics will experience its own 'LLM moment.'

This repository will be continuously updated, and we warmly invite contributions from the community. If you have any papers, projects, or resources that are not yet included, please feel free to submit them via a pull request or open an issue for discussion.

Let's build a comprehensive resource for the robotics and AI community!  (Modify!)

## To do list
1. æ±‡æ€»ç»¼è¿°é‡Œè¾¹æ‰€æœ‰ç›¸å…³è®ºæ–‡çš„æ ‡é¢˜ã€å‡ºç‰ˆç‰©ï¼›æŒ‰ç…§ç»¼è¿°ä¸­çš„ç±»åˆ«å¡«å…¥[Summary-before-2025].æ›´å°çš„ç›®å½•ç”¨tagæ¥è¡¨ç¤ºï¼Œä¾‹å¦‚åŸºäºé‡å»ºçš„æ–¹æ³•å¯ä»¥ç”¨[![Info](https://img.shields.io/badge/Method-Reconstruction-blue)](#). åŸºäºå¼‚å¸¸åˆæˆçš„å¯ä»¥ç”¨[![Info](https://img.shields.io/badge/Method-Generation-yellow)](#). æ¯ä¸ªé¢œè‰²æœ‰åŒºåˆ«ä¸€äº›
2. [Summary-before-2025]ä¸­çš„æ¨¡æ¿ä¸º: Paper_name [[å‡ºç‰ˆç‰©]()] [[Code]()]  [![Info](https://img.shields.io/badge/Model-ViT_+_MemoryBank_+_Real3D--AD-blue)](#)
3. å…¶ä»–éƒ¨åˆ†çš„æ¨¡æ¿ä¸ºPaper_name [[paper]()] [[Code]()]  [![Info](https://img.shields.io/badge/Model-ViT_+_MemoryBank_+_Real3D--AD-blue)](#)
4. è°ƒç ”ä»Šå¹´å¼‚å¸¸æ£€æµ‹é¡¶ä¼šçš„ç§ç±»å’Œé¡ºåºï¼Œä¿®æ”¹ Quick Navigationï¼Œå¹¶æ›´æ–°åˆ°ä¸‹è¾¹çš„æ ‡é¢˜å†…ï¼Œæ‰“ä¸Štagï¼Œè¿™ä¸ªtagå¯ä»¥é•¿ä¸€äº›ï¼Œä¾‹å¦‚[![Info](https://img.shields.io/badge/Model-ViT_+2D+_CLIP-blue)](#)
5. wosä¸Šæ£€ç´¢æœŸåˆŠæ–‡ç« ï¼Œä»…2025å¹´å³å¯ï¼Œé‡ç‚¹åœ¨PAMIã€IJCVã€IEEE/ACM Transï¼Œä»¥åŠéƒ¨åˆ†è¿˜ä¸é”™çš„çˆ±æ€å”¯å°”æœŸåˆŠPRã€KBSã€AEIã€JMSã€CIIç­‰ï¼ˆè¦ç­›é€‰ï¼Œè´¨é‡å·®çš„ä¸éœ€è¦åˆ—å‡ºæ¥ï¼‰
6. ç»¼è¿°æ–‡ç« éœ€è¦å†æ£€ç´¢ä¸€ä¸‹ï¼Œå®æ—¶ä¿æŒæ›´æ–°
7. BenchMarkç›´æ¥å°†ç»¼è¿°é‡Œè¾¹çš„æ•°æ®é›†è¡¨æ ¼å¡«å……å³å¯
8. Recent Featured Paperså¯ä»¥æ”¾ä¸€äº›ç›®å‰æ¯”è¾ƒçƒ­é—¨çš„ï¼ˆå°¤å…¶æ˜¯æˆ‘ä»¬è‡ªå·±æœ€è¿‘æ–°å‘è¡¨çš„æ–‡ç« ï¼‰
9. Our-Publications é‡Œè¾¹æ”¾ä¸€äº›æˆ‘ä»¬å‘çš„æ¯”è¾ƒå¥½é¡¶ä¼šå’Œtransæ–‡ç« åšå®£ä¼ 



---
## ğŸ… Recent Featured Papers
- A Survey on Vision-Language-Action Models: An Action Tokenization Perspective [[paper](https://arxiv.org/pdf/2507.01925)]
- A Survey on Vision-Language-Action Models for Autonomous Driving [[paper](https://arxiv.org/pdf/2506.24044)] [[project](https://github.com/JohnsonJiang1996/Awesome-VLA4AD)]  
- Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends [[paper](https://arxiv.org/pdf/2506.20966)] [[project](https://github.com/AoqunJin/Awesome-VLA-Post-Training)]  
- Foundation Models in Robotics: Applications, Challenges, and the Future [[paper](https://arxiv.org/pdf/2312.07843)] [[project](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models)]

---
## ğŸ”— Quick Navigation
- [Our-Publications](#Our-Publications) | [Survey](#Survey) | [BenchMark](#BenchMark) | [AAAI-2026](#AAAI-2026) | [NIPS-2025](#NIPS-2025) | [ICCV-2025](#ICCV-2025) | [CVPR-2025](#CVPR-2025) | [IJCAI-2025](#IJCAI-2025) | [Journal-2025](#Journal-2025) | [Summary-before-2025](#Summary-before-2025)

---




## Survey

<details open>
<summary>ğŸ“š Show/Hide Survey Papers</summary>

- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

</details>

---


## BenchMark

|  Name  |   Venue  |   Date   |   Source   |   Modality   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| MVTec | XXXX | 2022 | paper name+link | image |
| MVTec | XXXX | 2022 | paper name+link | image |
| MVTec | XXXX | 2022 | paper name+link | image |
| MVTec | XXXX | 2022 | paper name+link | image |





---




## **Our-Publications**
<details open>
<summary>ğŸ“š Show/Hide 2024 Papers</summary>

Template: Paper_name [[Paper]()] [[Code]()]  [![Info](https://img.shields.io/badge/Model-ViT_+_MemoryBank_+_Real3D--AD-blue)](#)
 - A Survey on Vision-Language-Action Models: An Action Tokenization Perspective. [[Paper]()] [[Code]()] [![Info](https://img.shields.io/badge/Model-ViT_+_MemoryBank_+_Real3D--AD-blue)](#)




</details>

---





## AAAI-2026

<details open>
<summary>ğŸ“š Show/Hide AAAI-2026 Papers</summary>


- [2025] Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey [[paper](https://arxiv.org/pdf/2508.13073)] [[project](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)]  [![AAAA](https://img.shields.io/badge/AAAA-BBBBB-blue)](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)
- [2025] Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning [[paper](https://arxiv.org/pdf/2508.10399)] [![Info](https://img.shields.io/badge/Model-ViT_+_MemoryBank_+_Real3D--AD-blue?style=flat-square)](#)
- [2025] Foundation Model Driven Robotics: A Comprehensive Review [[paper](https://arxiv.org/pdf/2507.10087)]
- [2025] [**PKU-PsiBot**] A Survey on Vision-Language-Action Models: An Action Tokenization Perspective [[paper](https://arxiv.org/pdf/2507.01925)]
- [2025] A Survey on Vision-Language-Action Models for Autonomous Driving [[paper](https://arxiv.org/pdf/2506.24044)] [[project](https://github.com/JohnsonJiang1996/Awesome-VLA4AD)]  
- [2025] Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends [[paper](https://arxiv.org/pdf/2506.20966)] [[project](https://github.com/AoqunJin/Awesome-VLA-Post-Training)]  
- [2025] [**IJRR 25**] Foundation Models in Robotics: Applications, Challenges, and the Future [[paper](https://arxiv.org/pdf/2312.07843)] [[project](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models)]
- [2025] Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes [[paper](https://www.arxiv.org/abs/2408.03539)]
- [2025] A Survey on Diffusion Policy for Robotic Manipulation: Taxonomy, Analysis, and Future Directions [[paper](https://doi.org/10.36227/techrxiv.174378343.39356214/v1)] [[project](https://github.com/HITSZ-Robotics/DiffusionPolicy-Robotics)]  
- [2025] Embodied Intelligent Industrial Robotics: Concepts and Techniques [[paper](https://arxiv.org/pdf/2505.09305)] [[project](https://github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List)] 
- [2025] Neural Brain: A Neuroscience-inspired Framework for Embodied Agents [[paper](https://arxiv.org/pdf/2505.07634)] [[project](https://github.com/CNJianLiu/Neural-Brain-for-Embodied-Agents)] 
- [2025] Vision-Language-Action Models: Concepts, Progress, Applications and Challenges [[paper](https://arxiv.org/pdf/2505.04769v1)]
- [2025] A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI [[paper](https://arxiv.org/pdf/2505.01458)]
- [2025] Multimodal Perception for Goal-oriented Navigation: A Survey [[paper](https://arxiv.org/pdf/2504.15643)]
- [2025] Diffusion Models for Robotic Manipulation: A Survey [[paper](https://arxiv.org/pdf/2504.08438)]
- [2025] Dexterous Manipulation through Imitation Learning: A Survey [[paper](https://arxiv.org/pdf/2504.03515)]
- [2025] Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision [[paper](https://arxiv.org/pdf/2504.02477)] [[project](https://github.com/Xiaofeng-Han-Res/MF-RV)] 
- [2025] SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey [[paper](https://arxiv.org/pdf/2503.09829)]
- [2025] Generative Artificial Intelligence in Robotic Manipulation: A Survey [[paper](https://arxiv.org/pdf/2503.03464)] [[project](https://github.com/GAI4Manipulation/AwesomeGAIManipulation)] 
- [2025] Development Report of Embodied Intelligence (Chinese) [[paper](https://www.caict.ac.cn/kxyj/qwfb/bps/202408/P020240830312499650772.pdf)]
- [2025] Survey on Vision-Language-Action Models [[paper](https://arxiv.org/pdf/2502.06851)]
- [2025] Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions [[paper](https://arxiv.org/pdf/2502.15336)]
- [2024] Embodied-AI with large models: research and challenges [[paper](https://www.sciengine.com/SSI/doi/10.1360/SSI-2024-0076)]
- [2024] A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- [2024] A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- [2024] Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- [2024] Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]


</details>

---

## NIPS-2025

<details open>
<summary>ğŸ“š Show/Hide NIPS-2025 Papers</summary>

- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

</details>

---



## ICCV-2025

<details open>
<summary>ğŸ“š Show/Hide ICCV-2025 Papers</summary>

- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

</details>

---



## CVPR-2025

<details open>
<summary>ğŸ“š Show/Hide CVPR-2025 Papers</summary>

- DFM: Differentiable Feature Matching for Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_DFM_Differentiable_Feature_Matching_for_Anomaly_Detection_CVPR_2025_paper.pdf)] [[code](https://github.com/wyattxuanyang/DFM)]  [![Info](https://img.shields.io/badge/Model-unsupervised_+_memorybank_+_MVTecâ€“AD_and_VisA-blue)](#)
- Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_Dual-Interrelated_Diffusion_Model_for_Few-Shot_Anomaly_Image_Generation_CVPR_2025_paper.pdf)] [[code](https://github.com/yinyjin/DualAnoDiff)] [![Info](https://img.shields.io/badge/Model-fewâ€“shot_+_diffusion_+_MVTec-blue)](#)
- AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial Scenarios [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_AnomalyNCD_Towards_Novel_Anomaly_Class_Discovery_in_Industrial_Scenarios_CVPR_2025_paper.pdf)] [[code](https://github.com/HUST-SLOW/AnomalyNCD)]  [![Info](https://img.shields.io/badge/Model-self_supervised_+_MVTecâ€“AD_and_MTD-blue)](#)
- PIAD: Pose and Illumination agnostic Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_PIAD_Pose_and_Illumination_agnostic_Anomaly_Detection_CVPR_2025_paper.pdf)] [[code](https://github.com/Kaichen-Yang/piad_baseline)]  [![Info](https://img.shields.io/badge/Model-3DGS_representation_+_MAD_and_SYNT-blue)](#)
- Distribution Prototype Diffusion Learning for Open-set Supervised Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Distribution_Prototype_Diffusion_Learning_for_Open-set_Supervised_Anomaly_Detection_CVPR_2025_paper.pdf)] [[code](https://github.com/fuyunwang/DPDL)]  [![Info](https://img.shields.io/badge/Model-supervised_+_diffusion_+_MVTecâ€“AD_Optical_SDD_AITEX_ELPV_Mastcam-blue)](#)
- MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_MANTA_A_Large-Scale_Multi-View_and_Visual-Text_Anomaly_Detection_Dataset_for_CVPR_2025_paper.pdf)] [[code](https://grainnet.github.io/MANTA)]  [![Info](https://img.shields.io/badge/Model-multiview_dataset-blue)](#)
- Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Nekrasov_Spotting_the_Unexpected_STU_A_3D_LiDAR_Dataset_for_Anomaly_CVPR_2025_paper.pdf)] [[code](https://github.com/kumuji/stu_dataset)]  [![Info](https://img.shields.io/badge/Model-anomaly_segmentation_dataset-blue)](#)
- UniNet: A Contrastive Learning-guided Unified Framework with Feature Selection for Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_UniNet_A_Contrastive_Learning-guided_Unified_Framework_with_Feature_Selection_for_CVPR_2025_paper.pdf)] [[code](https://github.com/pangdaTangtt/UniNet)]  [![Info](https://img.shields.io/badge/Model-unsupervised_and_supervised_+_studentâ€“teacher_models_+_MVTecâ€“AD_BTAD_MVTecâ€“3D_VAD-blue)](#)
- Real-IAD D3: A Real-World 2D/Pseudo-3D Dataset for Industrial Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Real-IAD_D3_A_Real-World_2DPseudo-3D3D_Dataset_for_Industrial_Anomaly_Detection_CVPR_2025_paper.pdf)] [[code](https://github.com/Real-IAD-D3/main)]  [![Info](https://img.shields.io/badge/Model-unsupervised_+_Realâ€“IAD_DÂ³-blue)](#)
- Dinomaly: The Less is More Philosophy in Multi-Class Unsupervised Anomaly Detection [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Dinomaly_The_Less_Is_More_Philosophy_in_Multi-Class_Unsupervised_Anomaly_CVPR_2025_paper.pdf)] [[code](https://github.com/guojiajeremy/Dinomaly)]  [![Info](https://img.shields.io/badge/Model-diffusion_+_MVTecâ€“AD_and_Visa-blue)](#)
- Unseen Visual Anomaly Generation [[paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Unseen_Visual_Anomaly_Generation_CVPR_2025_paper.pdf)] [[code](https://github.com/EPFL-IMOS/AnomalyAny)]  [![Info](https://img.shields.io/badge/Model-unsupervised_+_memorybank_+_MVTecâ€“AD_and_VisA-blue)](#)


</details>

---


## IJCAI-2025

<details open>
<summary>ğŸ“š Show/Hide IJCAI-2025 Papers</summary>

- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

</details>

---



## Journal-2025

<details open>
<summary>ğŸ“š Show/Hide Journal-2025 Papers</summary>

- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

</details>

---










## Summary-before-2025

### 2D ANOMALY DETECTION
#### ğŸ“ŠUnsupervised 
- Imagenet classification with deep convolutional neural networks [[paper](https://dl.acm.org/doi/pdf/10.1145/3065386)]  [[code](https://github.com/dansuh17/alexnet-pytorch?tab=readme-ov-file)]  [![Info](https://img.shields.io/badge/Model-CNN-blue)](#)
- Fast Râ€“CNN [[paper](https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)]  [[code](https://github.com/rbgirshick/fast-rcnn)]  [![Info](https://img.shields.io/badge/Model-Fast_Râ€“CNN-blue)](#)
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]



#### ğŸ“ŠSemi-supervised
- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]


#### ğŸ“ŠZero/Few-shot 
- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]


---
### 3D ANOMALY DETECTION

#### ğŸ“ŠPoint Cloud
- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]


#### ğŸ“ŠRGBD
- A Survey on Vision-Language-Action Models for Embodied AI [[paper](https://arxiv.org/abs/2405.14093)]
- A Survey of Embodied Learning for Object-Centric Robotic Manipulation [[paper](https://arxiv.org/pdf/2408.11537)]
- Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI [[paper](https://arxiv.org/pdf/2407.06886)]
- Vision-language navigation: a survey and taxonomy [[paper](https://arxiv.org/pdf/2108.11544)]

---






## BibTex Citation

If you find this paper and repository useful, please cite our paperâ˜ºï¸.

```
@article{ADSurvey,
  title={A comprehensive survey for real-world industrial defect detection: Challenges, approaches, and prospects},
  author={Cheng, Yuqi and Cao, Yunkang and Yao, Haiming and Luo, Wei and Jiang, Cheng and Zhang, Hui and Shen, Weiming},
  journal={arXiv preprint arXiv:2507.13378},
  year={2025}
}
```
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hustCYQ/Towards-Practical-Industrial-Anomaly-Detection&type=Date)](https://www.star-history.com/#hustCYQ/Towards-Practical-Industrial-Anomaly-Detection&Date)
























